{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b234cc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Successfully imported gridfm_graphkit modules!\n"
     ]
    }
   ],
   "source": [
    "# Test import from gridfm-graphkit to verify the path setup works \n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../../gridfm-graphkit')) #Replace with the correct path\n",
    "\n",
    "try:\n",
    "    from gridfm_graphkit.datasets.powergrid_datamodule import LitGridDataModule\n",
    "    from gridfm_graphkit.io.param_handler import NestedNamespace\n",
    "    from gridfm_graphkit.tasks.feature_reconstruction_task import FeatureReconstructionTask\n",
    "    from gridfm_graphkit.utils.visualization import (\n",
    "        visualize_error,\n",
    "        visualize_quantity_heatmap,\n",
    "    )\n",
    "    from gridfm_graphkit.datasets.globals import PD, QD, PG, QG, VM, VA\n",
    "    print(\"✓ Successfully imported gridfm_graphkit modules!\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Import failed: {e}\")\n",
    "    print(\"Make sure you're running this notebook from the correct directory and gridfm-graphkit is properly installed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a759bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from gridfm-datakit to access process_network functions\n",
    "# ## datakit should be in opf branch ##\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('../../gridfm-datakit')) #Replace with the correct path\n",
    "\n",
    "try:\n",
    "    from gridfm_datakit.process.process_network import (\n",
    "        network_preprocessing,\n",
    "        pf_preprocessing,\n",
    "        pf_post_processing,\n",
    "        process_scenario_unsecure,\n",
    "        process_scenario_chunk,\n",
    "        process_scenario_secure\n",
    "    )\n",
    "    from gridfm_datakit.process.solvers import run_pf, run_opf\n",
    "    from gridfm_datakit.save import save_node_edge_data\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Import failed: {e}\")\n",
    "    print(\"Make sure you're running this notebook from the correct directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27b0384d-74ff-4019-a901-cab81d502f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEMONSTRATE GRIDSEARCH WITH CASE14\n",
    "# https://dwightreid.com/site/power-system-contingency-analysis-with-python-pandapower/\n",
    "\n",
    "import pandas as pd\n",
    "import pandapower as pp\n",
    "import pandapower.networks as ppnets\n",
    "#from pandapower.plotting import simple_plot, simple_plotly, pf_res_plotly\n",
    "import pandapower.plotting as plt\n",
    "import pandapower.plotting.plotly as pplotly\n",
    "import matplotlib.pyplot as mplt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5209986-da51-4979-9a25-5fed71058fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "This pandapower network includes the following parameter tables:\n",
       "   - bus (14 elements)\n",
       "   - load (11 elements)\n",
       "   - gen (4 elements)\n",
       "   - shunt (1 element)\n",
       "   - ext_grid (1 element)\n",
       "   - line (15 elements)\n",
       "   - trafo (5 elements)\n",
       "   - poly_cost (5 elements)\n",
       "   - bus_geodata (14 elements)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net =  ppnets.case14() \n",
    "net.ext_grid['in_service']=True # disconnect from external grid - create an island\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50da3bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.bus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd04b79-92b9-4e7b-b706-2f3f05892250",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.simple_plot(net,  plot_loads=True, plot_gens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba76bfa-2931-492d-998d-c8a602f39bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.gen['vm_pu']=1.045\n",
    "\n",
    "# Perform a generator dispatch by maxing out the first three generators and setting the fourth one as the slack.\n",
    "# net.gen.loc[0,'p_mw'] = 140 #1\n",
    "# net.gen.loc[1,'p_mw'] = 100 #2\n",
    "# net.gen.loc[2,'p_mw'] = 100 #5\n",
    "# net.gen.loc[3,'slack'] = True #7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c5e22d-d246-4449-8b02-03b37c047aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617ffe6c-070b-4e2d-afef-b552c1b64df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec55ae78-29ab-471f-a723-f9dad7c081f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pp.runpp(net,numba=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41da019e-da6b-479b-bcaa-80510d11659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.res_bus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff776a9-7db0-4840-a025-cd3d5c3bc78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real power: gen ?= load + shunt(consumption) + xward + losses + export\n",
    "#P_rhs = P_load + max(P_shunt, 0.0) + P_xward + P_loss + P_export\n",
    "#Q_rhs = Q_load + Q_loss + Q_shunt + Q_xward + Q_export\n",
    "\n",
    "# the basic way\n",
    "print('P_load = ' ,net.res_load['p_mw'].sum())\n",
    "print('P_gen = ' ,net.res_gen['p_mw'].sum()+net.res_sgen['p_mw'].sum()+net.res_ext_grid['p_mw'].sum())\n",
    "print('P_shunt = ',net.res_shunt['p_mw'].sum())\n",
    "print('P_loss = ',net.res_line['pl_mw'].sum()+net.res_trafo['pl_mw'].sum())\n",
    "print('***************************')\n",
    "print('Q_load = ' ,net.res_load['q_mvar'].sum())\n",
    "print('Q_gen = ' ,net.res_gen['q_mvar'].sum()+net.res_sgen['q_mvar'].sum()+net.res_ext_grid['q_mvar'].sum())\n",
    "print('Q_shunt = ',net.res_shunt['q_mvar'].sum())\n",
    "print('Q_loss = ',net.res_line['ql_mvar'].sum()+net.res_trafo['ql_mvar'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d8b447",
   "metadata": {},
   "source": [
    "Replace with gridfm finetuned model (case 14 now, later texas grid)\n",
    "Best way to visualise the lo-voltages, high voltages and line overloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170de1ff-0221-47ce-94c7-db612344da58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "import pandapower as pp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "scenarios = pd.read_excel(\"../data/load-scenarios.xlsx\")\n",
    "added_load_values = [4,12,20]\n",
    "q_p_ratio = 0.5\n",
    "vmax, vmin = 1.05, 0.95\n",
    "line_loading_max = 1.0\n",
    "\n",
    "lines = net.line.index\n",
    "bus_to_load = {int(row.bus): idx for idx, row in net.load.iterrows()}\n",
    "\n",
    "summary_records = [] #counts of violation per scenario, bus, added load\n",
    "detail_records = [] #detailed records of violations per scenario, bus, added load, line dropped\n",
    "processed_data = [] #processed data from pandapower\n",
    "\n",
    "# network_preprocessing(net) #imported from gridfm_datakit.process.process_network\n",
    "\n",
    "notConvergedCount = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf44f784",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LOOP 1: iterate over load scenarios\n",
    "# ============================================================\n",
    "for i,scen_id in enumerate(scenarios['scenario'].unique()):\n",
    "    if i > 4:\n",
    "        break\n",
    "    scen_data = scenarios[scenarios['scenario'] == scen_id]\n",
    "    print(f\"\\n=== Scenario {scen_id} ===\")\n",
    "\n",
    "    # set base case\n",
    "    for _, row in scen_data.iterrows():\n",
    "        bus = int(row['bus'])\n",
    "        if bus in bus_to_load:\n",
    "            load_idx = bus_to_load[bus]\n",
    "            net.load.loc[load_idx, 'p_mw'] = row['p_mw']\n",
    "            net.load.loc[load_idx, 'q_mvar'] = row['q_mvar']\n",
    "\n",
    "    # ============================================================\n",
    "    # LOOP 2: over buses\n",
    "    # ============================================================\n",
    "    for bus in scen_data['bus'].unique():\n",
    "        if bus not in bus_to_load:\n",
    "            continue\n",
    "        load_idx = bus_to_load[bus]\n",
    "        # print(f\"  -> Bus {bus}\")\n",
    "\n",
    "        # ============================================================\n",
    "        # LOOP 3: load increments\n",
    "        # ============================================================\n",
    "        for added_load in added_load_values:\n",
    "\n",
    "            # Apply incremental load\n",
    "            net.load.loc[load_idx, 'p_mw'] += added_load\n",
    "            net.load.loc[load_idx, 'q_mvar'] += added_load * q_p_ratio\n",
    "\n",
    "            run_opf(net)\n",
    "            # network_preprocessing(net) #imported from gridfm_datakit.process.process_network\n",
    "            pf_preprocessing(net)  \n",
    "\n",
    "            # Track how many contingencies violated limits\n",
    "            lv_count = hv_count = ol_count = 0\n",
    "            # print(\"added_load\",added_load,\"\\n\")\n",
    "\n",
    "            # ============================================================\n",
    "            # LOOP 4: contingency loop (line outages)\n",
    "            # ============================================================\n",
    "            for l in lines:\n",
    "                net.line.loc[l, 'in_service'] = False\n",
    "                try:\n",
    "                    # pp.runpp(net, numba=True, max_iteration=100)\n",
    "                    run_pf(net)\n",
    "                    pf_data = pf_post_processing(net)\n",
    "                        \n",
    "                    processed_data.append((\n",
    "                        pf_data['bus'],\n",
    "                        pf_data['gen'],\n",
    "                        pf_data['branch'],\n",
    "                        pf_data['Y_bus'],\n",
    "                    ))\n",
    "\n",
    "                except pp.LoadflowNotConverged:\n",
    "                    notConvergedCount += 1\n",
    "                    # record as special case\n",
    "                    detail_records.append({\n",
    "                        'scenario': scen_id,\n",
    "                        'bus': bus,\n",
    "                        'added_load': added_load,\n",
    "                        'line_dropped': l,\n",
    "                        'lv_nodes': {},\n",
    "                        'hv_nodes': {},\n",
    "                        'overloaded_lines': {}\n",
    "                    })\n",
    "                    net.line.loc[l, 'in_service'] = True\n",
    "                    continue\n",
    "\n",
    "                vm = net.res_bus.vm_pu\n",
    "                line_loading = net.res_line.loading_percent / 100.0\n",
    "\n",
    "                # --- Check voltage limits ---\n",
    "                hv_nodes = {}\n",
    "                if vm.max() > vmax:\n",
    "                    hv_count += 1\n",
    "                    viol_buses = vm.index[vm > vmax]\n",
    "                    for vb in viol_buses:\n",
    "                        hv_nodes[int(vb)] = float(vm.loc[vb])\n",
    "\n",
    "                lv_nodes = {}\n",
    "                if vm.min() < vmin:\n",
    "                    lv_count += 1\n",
    "                    viol_buses = vm.index[vm < vmin]\n",
    "                    for vb in viol_buses:\n",
    "                        lv_nodes[int(vb)] = float(vm.loc[vb])\n",
    "\n",
    "                # --- Check line overload ---\n",
    "                overloaded_lines = {}\n",
    "                if line_loading.max() > line_loading_max:\n",
    "                    ol_count += 1\n",
    "                    viol_lines = line_loading.index[line_loading > line_loading_max]\n",
    "                    for vl in viol_lines:\n",
    "                        overloaded_lines[int(vl)] = float(line_loading.loc[vl])\n",
    "                \n",
    "                # Record details for EVERY case (violations or not)\n",
    "                detail_records.append({\n",
    "                    'scenario': scen_id,\n",
    "                    'bus': bus,\n",
    "                    'added_load': added_load,\n",
    "                    'line_dropped': l,\n",
    "                    'lv_nodes': lv_nodes,              # dict of {node_id: vm_pu}\n",
    "                    'hv_nodes': hv_nodes,              # dict of {node_id: vm_pu}\n",
    "                    'overloaded_lines': overloaded_lines  # dict of {line_id: loading}\n",
    "                })\n",
    "\n",
    "                # restore line\n",
    "                net.line.loc[l, 'in_service'] = True\n",
    "\n",
    "            # --- Store summary for this (scenario, bus, increment) ---\n",
    "            summary_records.append({\n",
    "                'scenario': scen_id,\n",
    "                'bus': bus,\n",
    "                'added_load': added_load,\n",
    "                'low_voltage': lv_count,\n",
    "                'high_voltage': hv_count,\n",
    "                'overloading': ol_count,\n",
    "\n",
    "            })\n",
    "\n",
    "            # Reset load to base value\n",
    "            net.load.loc[load_idx, 'p_mw'] -= added_load\n",
    "            net.load.loc[load_idx, 'q_mvar'] -= added_load * q_p_ratio\n",
    "\n",
    "# ============================================================\n",
    "# Final DataFrames\n",
    "# ============================================================\n",
    "summary_df = pd.DataFrame(summary_records)\n",
    "details_df = pd.DataFrame(detail_records)\n",
    "\n",
    "print(\"\\n=== Summary Results ===\")\n",
    "print(summary_df.head())\n",
    "\n",
    "print(\"\\n=== Detailed Violations ===\")\n",
    "print(details_df.head()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84bdc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(processed_data))\n",
    "print(5*3*15*11)\n",
    "#15 lines, 11 buses, 2 scenarios, 2 added loads = 660"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d64722",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./case14_finetune.yaml\n",
    "\n",
    "seed: 42\n",
    "data:\n",
    "  networks: [\"case14_ieee\"]\n",
    "  scenarios: [300000]\n",
    "  normalization: \"baseMVAnorm\"\n",
    "  baseMVA: 100\n",
    "  mask_type: \"pf\"\n",
    "  mask_value: 0.0\n",
    "  mask_ratio: 0.5\n",
    "  mask_dim: 6\n",
    "  learn_mask: False\n",
    "  val_ratio: 0.0\n",
    "  test_ratio: 0.1\n",
    "  workers: 16\n",
    "model:\n",
    "  attention_head: 8\n",
    "  dropout: 0.1\n",
    "  edge_dim: 2\n",
    "  hidden_size: 256\n",
    "  input_dim: 9\n",
    "  num_layers: 8\n",
    "  output_dim: 6\n",
    "  pe_dim: 20\n",
    "  type: GPSTransformer\n",
    "training:\n",
    "  batch_size: 32\n",
    "  epochs: 50\n",
    "  losses: [\"MaskedMSE\", \"PBE\"]\n",
    "  loss_weights: [0.9, 0.1]\n",
    "  accelerator: auto\n",
    "  devices: auto\n",
    "  strategy: auto\n",
    "optimizer:\n",
    "  learning_rate: 0.0001\n",
    "  beta1: 0.9\n",
    "  beta2: 0.999\n",
    "  lr_decay: 0.7\n",
    "  lr_patience: 10\n",
    "callbacks:\n",
    "  patience: 20\n",
    "  tol: 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd51dcbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be32fdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dir_path = \"/Users/srihi/Study/RA/gridfm-datakit/data_test/case14_ieee/raw/\"\n",
    "os.makedirs(dir_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72149e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_node_edge_data(\n",
    "    net=net,  # The network object\n",
    "    node_path=dir_path+\"pf_node.csv\",     # Path for bus/node data\n",
    "    branch_path=dir_path+\"pf_edge_backup.csv\",   # Path for branch/edge data\n",
    "    gen_path=dir_path+\"pf_gen.csv\",       # Path for generator data\n",
    "    y_bus_path=dir_path+\"pf_edge.csv\",    # Path for Y-bus data\n",
    "    processed_data=processed_data,  # Your list of tuples\n",
    "    dcpf=False  # Set to True if you want DC power flow columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70385101",
   "metadata": {},
   "source": [
    "Paste the 4 files generated above into the data_test folder, and run the below code to get predictions\n",
    "\n",
    "Modify the path of data_test as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f841eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68faab19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../../gridfm-graphkit')) #Replace with the correct path\n",
    "import torch\n",
    "import lightning as L\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from gridfm_graphkit.io.param_handler import NestedNamespace\n",
    "from gridfm_graphkit.datasets.powergrid_datamodule import LitGridDataModule\n",
    "from gridfm_graphkit.tasks.feature_reconstruction_task import FeatureReconstructionTask\n",
    "\n",
    "config_path = \"./case14_finetune.yaml\"\n",
    "#Replace with the correct path for the finetuned model\n",
    "model_path = \"../../mlruns/643070220719696069/4aa50bba5a3748e0919b6cbb40e27785/artifacts/model/best_finetuned_model1103.pt\" \n",
    "# data_path = \"./../data/\"\n",
    "data_path = \"../../gridfm-datakit/data_test\"\n",
    "output_path = data_path+\"/predictions\"\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Load config\n",
    "with open(config_path, \"r\") as f:\n",
    "    config_dict = yaml.safe_load(f)\n",
    "config_args = NestedNamespace(**config_dict)\n",
    "torch.manual_seed(config_args.seed)\n",
    "random.seed(config_args.seed)\n",
    "np.random.seed(config_args.seed)\n",
    "\n",
    "litGrid = LitGridDataModule(config_args, data_path)\n",
    "litGrid.setup(\"test\")\n",
    "\n",
    "model = FeatureReconstructionTask(\n",
    "    config_args,\n",
    "    litGrid.node_normalizers,\n",
    "    litGrid.edge_normalizers,\n",
    ")\n",
    "\n",
    "print(f\"Loading model weights from {model_path}\")\n",
    "state_dict = torch.load(model_path, map_location='cpu')\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    accelerator='cpu',\n",
    "    devices=1,\n",
    "    logger=False,  \n",
    ")\n",
    "\n",
    "predictions = trainer.predict(model=model, datamodule=litGrid)\n",
    "\n",
    "all_outputs = []\n",
    "all_scenarios = []\n",
    "all_bus_numbers = []\n",
    "\n",
    "for batch in predictions:\n",
    "    all_outputs.append(batch[\"output\"])\n",
    "    all_scenarios.append(batch[\"scenario_id\"])\n",
    "    all_bus_numbers.append(batch[\"bus_number\"])\n",
    "\n",
    "outputs = np.concatenate(all_outputs, axis=0)\n",
    "scenario_ids = np.concatenate(all_scenarios, axis=0)\n",
    "bus_numbers = np.concatenate(all_bus_numbers, axis=0)\n",
    "\n",
    "predictions_df = pd.DataFrame({\n",
    "    \"scenario\": scenario_ids,\n",
    "    \"bus\": bus_numbers,\n",
    "    \"PD_pred\": outputs[:, 0],\n",
    "    \"QD_pred\": outputs[:, 1],\n",
    "    \"PG_pred\": outputs[:, 2],\n",
    "    \"QG_pred\": outputs[:, 3],\n",
    "    \"VM_pred\": outputs[:, 4],\n",
    "    \"VA_pred\": outputs[:, 5],\n",
    "})\n",
    "\n",
    "# Save predictions\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "csv_path = os.path.join(output_path, \"predictions.csv\")\n",
    "predictions_df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"Saved predictions to {csv_path}\")\n",
    "print(f\"Predictions shape: {predictions_df.shape}\")\n",
    "print(\"\\nFirst few predictions:\")\n",
    "print(predictions_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a405f3",
   "metadata": {},
   "source": [
    "Compare the predicted values with the actual values from pf_node.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89b745b",
   "metadata": {},
   "source": [
    "Distribution of train and test data is not same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eac1945",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(litGrid.node_normalizers[0].baseMVA)\n",
    "print(litGrid.edge_normalizers[0].baseMVA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daa9532",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLOT THE ERRORS between actual and predicted values\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "pf_node = pd.read_csv(\"../../gridfm-datakit/data_test/case14_ieee/raw/pf_node.csv\")\n",
    "predictions = pd.read_csv(\"../../gridfm-datakit/data_test/predictions/predictions.csv\")\n",
    "# pf_node = pd.read_csv(\"/Users/srihi/Study/RA/gridfm-datakit/data_out/case14_ieee/raw/pf_node.csv\")\n",
    "# predictions = pd.read_csv(\"/Users/srihi/Study/RA/predictions.csv\")\n",
    "\n",
    "gen_buses = [0, 1, 2, 5, 7] \n",
    "load_buses = [b for b in pf_node['bus'].unique() if b not in gen_buses]\n",
    "\n",
    "print(f\"Generator buses (excluded): {gen_buses}\")\n",
    "print(f\"Load buses (compared): {load_buses}\")\n",
    "\n",
    "# Merge the dataframes\n",
    "comparison = pf_node.merge(predictions, on=['scenario', 'bus'], how='inner')\n",
    "\n",
    "comparison = comparison[comparison['bus'].isin(load_buses)]\n",
    "\n",
    "comparison['VM_error'] = abs(comparison['Vm'] - comparison['VM_pred'])\n",
    "comparison['VA_error'] = abs(comparison['Va'] - comparison['VA_pred'])\n",
    "comparison['PD_error'] = abs(comparison['Pd'] - comparison['PD_pred'])\n",
    "comparison['QD_error'] = abs(comparison['Qd'] - comparison['QD_pred'])\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n=== Error Summary (Load Buses Only) ===\")\n",
    "print(f\"\\nVM Error Statistics:\")\n",
    "print(comparison['VM_error'].describe())\n",
    "print(f\"\\nVA Error Statistics:\")\n",
    "print(comparison['VA_error'].describe())\n",
    "\n",
    "# Per-bus analysis\n",
    "print(\"\\n=== Per-Bus VM Error (Load Buses) ===\")\n",
    "bus_summary = comparison.groupby('bus').agg({\n",
    "    'VM_error': ['mean', 'std', 'max'],\n",
    "    'VA_error': ['mean', 'std', 'max'],\n",
    "}).round(6)\n",
    "print(bus_summary)\n",
    "\n",
    "# Save comparison results\n",
    "comparison_results = comparison[['scenario', 'bus', 'Vm', 'VM_pred', 'VM_error', \n",
    "                                  'Va', 'VA_pred', 'VA_error', 'Pd', 'PD_pred', 'PD_error',\n",
    "                                  'Qd', 'QD_pred', 'QD_error']]\n",
    "comparison_results.to_csv('vm_comparison_load_buses.csv', index=False)\n",
    "print(f\"\\nSaved detailed comparison to vm_comparison_load_buses.csv\")\n",
    "\n",
    "# Optional: Plot VM errors\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "for bus in load_buses:\n",
    "    bus_data = comparison[comparison['bus'] == bus]['VM_error']\n",
    "    plt.boxplot(bus_data, positions=[bus], widths=0.6)\n",
    "plt.xlabel('Bus')\n",
    "plt.ylabel('VM Absolute Error')\n",
    "plt.title('VM Error Distribution by Load Bus')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(comparison['VM_error'], bins=50, edgecolor='black')\n",
    "plt.xlabel('VM Absolute Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('VM Error Distribution (All Load Buses)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('vm_error_analysis.png', dpi=150)\n",
    "print(\"Saved error plots to vm_error_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c968ca2",
   "metadata": {},
   "source": [
    "BELOW CODE NOT USED, JUST PUT FOR REFERENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ebe213",
   "metadata": {},
   "source": [
    "Compare violations between predicted and actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712f6e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # Read data\n",
    "# # pf_edge = pd.read_csv(dir_path+\"/pf_edge.csv\")\n",
    "# pf_node = pd.read_csv(dir_path+\"/pf_node.csv\")\n",
    "# predictions = pd.read_csv(\"../data/case14_ieee/predictions/predictions.csv\")\n",
    "\n",
    "# # Parameters\n",
    "# vmax, vmin = 1.05, 0.95\n",
    "# line_loading_max = 1.0\n",
    "# gen_buses = [1, 2, 5, 7]  # Removed 0\n",
    "\n",
    "# print(f\"Unique scenarios in predictions: {predictions['scenario'].nunique()}\")\n",
    "\n",
    "# # Get scenarios that exist in BOTH files\n",
    "# common_scenarios = sorted(set(pf_node['scenario'].unique()) & set(predictions['scenario'].unique()))\n",
    "\n",
    "\n",
    "# # Analyze Pandapower results for COMMON scenarios only\n",
    "# pp_violations = []\n",
    "# for scenario in common_scenarios:\n",
    "#     scenario_buses = pf_node[pf_node['scenario'] == scenario]\n",
    "#     # scenario_lines = pf_edge[pf_edge['scenario'] == scenario]\n",
    "    \n",
    "#     # Only check voltage on load buses\n",
    "#     load_buses = scenario_buses[~scenario_buses['bus'].isin(gen_buses)]\n",
    "#     vm = load_buses['Vm'].values\n",
    "    \n",
    "#     # Count voltage violations\n",
    "#     lv_count = np.sum(vm < vmin)\n",
    "#     hv_count = np.sum(vm > vmax)\n",
    "    \n",
    "#     # Count line overloading\n",
    "#     # in_service = scenario_lines['br_status'] == 1.0\n",
    "#     # active_lines = scenario_lines[in_service]\n",
    "    \n",
    "#     pp_violations.append({\n",
    "#         'scenario': scenario,\n",
    "#         'pp_lv_count': lv_count,\n",
    "#         'pp_hv_count': hv_count\n",
    "#     })\n",
    "\n",
    "# pp_violations_df = pd.DataFrame(pp_violations)\n",
    "\n",
    "# # Analyze GridFM predictions for COMMON scenarios only\n",
    "# gf_violations = []\n",
    "# for scenario in common_scenarios:\n",
    "#     scenario_data = predictions[predictions['scenario'] == scenario]\n",
    "    \n",
    "#     # Only check voltage on load buses\n",
    "#     load_data = scenario_data[~scenario_data['bus'].isin(gen_buses)]\n",
    "#     vm_pred = load_data['VM_pred'].values\n",
    "    \n",
    "#     # Count voltage violations\n",
    "#     lv_count = np.sum(vm_pred < vmin)\n",
    "#     hv_count = np.sum(vm_pred > vmax)\n",
    "    \n",
    "#     gf_violations.append({\n",
    "#         'scenario': scenario,\n",
    "#         'gf_lv_count': lv_count,\n",
    "#         'gf_hv_count': hv_count,\n",
    "#     })\n",
    "\n",
    "# gf_violations_df = pd.DataFrame(gf_violations)\n",
    "\n",
    "# # Merge and compare\n",
    "# comparison = pp_violations_df.merge(gf_violations_df, on='scenario', how='inner')\n",
    "# comparison['lv_mismatch'] = comparison['pp_lv_count'] != comparison['gf_lv_count']\n",
    "# comparison['hv_mismatch'] = comparison['pp_hv_count'] != comparison['gf_hv_count']\n",
    "\n",
    "# # Get scenarios with mismatches\n",
    "# mismatch_mask = comparison['lv_mismatch'] | comparison['hv_mismatch']\n",
    "# mismatched_scenarios = comparison[mismatch_mask]['scenario'].tolist()\n",
    "\n",
    "# # Print violation details for ALL mismatched scenarios\n",
    "# print(\"\\n=== Investigating Violations in Mismatched Scenarios ===\\n\")\n",
    "\n",
    "# violation_details = []\n",
    "\n",
    "# for scenario in mismatched_scenarios:\n",
    "#     pp_data = pf_node[pf_node['scenario'] == scenario]\n",
    "#     gf_data = predictions[predictions['scenario'] == scenario]\n",
    "    \n",
    "#     # Filter load buses only\n",
    "#     pp_load = pp_data[~pp_data['bus'].isin(gen_buses)]\n",
    "#     gf_load = gf_data[~gf_data['bus'].isin(gen_buses)]\n",
    "    \n",
    "#     # Find low voltage buses in Pandapower\n",
    "#     pp_lv_mask = pp_load['Vm'] < vmin\n",
    "#     pp_hv_mask = pp_load['Vm'] > vmax\n",
    "    \n",
    "#     # Check for violations\n",
    "#     if pp_lv_mask.any():\n",
    "#         print(f\"--- Scenario {scenario}: LOW VOLTAGE violations ---\")\n",
    "#         for _, row in pp_load[pp_lv_mask].iterrows():\n",
    "#             bus = row['bus']\n",
    "#             pp_vm = row['Vm']\n",
    "#             gf_row = gf_load[gf_load['bus'] == bus]\n",
    "#             if len(gf_row) > 0:\n",
    "#                 gf_vm = gf_row.iloc[0]['VM_pred']\n",
    "#                 error = pp_vm - gf_vm\n",
    "#                 print(f\"  Bus {bus}: PP_VM={pp_vm:.6f}, GF_VM={gf_vm:.6f}, Error={error:.6f}\")\n",
    "#                 violation_details.append({\n",
    "#                     'scenario': scenario,\n",
    "#                     'bus': bus,\n",
    "#                     'violation_type': 'LV',\n",
    "#                     'pp_value': pp_vm,\n",
    "#                     'gf_value': gf_vm,\n",
    "#                     'error': error\n",
    "#                 })\n",
    "    \n",
    "#     if pp_hv_mask.any():\n",
    "#         print(f\"--- Scenario {scenario}: HIGH VOLTAGE violations ---\")\n",
    "#         for _, row in pp_load[pp_hv_mask].iterrows():\n",
    "#             bus = row['bus']\n",
    "#             pp_vm = row['Vm']\n",
    "#             gf_row = gf_load[gf_load['bus'] == bus]\n",
    "#             if len(gf_row) > 0:\n",
    "#                 gf_vm = gf_row.iloc[0]['VM_pred']\n",
    "#                 error = pp_vm - gf_vm\n",
    "#                 print(f\"  Bus {bus}: PP_VM={pp_vm:.6f}, GF_VM={gf_vm:.6f}, Error={error:.6f}\")\n",
    "#                 violation_details.append({\n",
    "#                     'scenario': scenario,\n",
    "#                     'bus': bus,\n",
    "#                     'violation_type': 'HV',\n",
    "#                     'pp_value': pp_vm,\n",
    "#                     'gf_value': gf_vm,\n",
    "#                     'error': error\n",
    "#                 })\n",
    "\n",
    "# # Print summary statistics\n",
    "# print(\"\\n=== Summary Statistics ===\")\n",
    "# print(f\"Scenarios with perfect agreement: {np.sum(~mismatch_mask)}\")\n",
    "# print(f\"Scenarios with LV count mismatches: {comparison['lv_mismatch'].sum()}\")\n",
    "# print(f\"Scenarios with HV count mismatches: {comparison['hv_mismatch'].sum()}\")\n",
    "# print(f\"Percentage agreement (LV): {100*np.mean(~comparison['lv_mismatch']):.2f}%\")\n",
    "# print(f\"Percentage agreement (HV): {100*np.mean(~comparison['hv_mismatch']):.2f}%\")\n",
    "\n",
    "# if violation_details:\n",
    "#     violation_df = pd.DataFrame(violation_details)\n",
    "#     print(f\"\\nMean error: {violation_df['error'].mean():.6f}\")\n",
    "#     print(f\"Max error: {violation_df['error'].max():.6f}\")\n",
    "#     print(f\"Min error: {violation_df['error'].min():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613080df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from gridfm_graphkit.models.gps_transformer import GPSTransformer\n",
    "import yaml\n",
    "\n",
    "config_path = \"case14_finetune.yaml\"\n",
    "model_path = \"../../gridfm-graphkit/examples/models/GridFM_v0_2.pth\"\n",
    "\n",
    "with open(config_path, \"r\") as f:\n",
    "    config_dict = yaml.safe_load(f)\n",
    "config_args = NestedNamespace(**config_dict)\n",
    "\n",
    "class SimpleModel(torch.nn.Module):\n",
    "    def __init__(self, config_args):\n",
    "        super().__init__()\n",
    "        self.model = GPSTransformer(config_args)\n",
    "        self.config = config_args\n",
    "        \n",
    "    def forward(self, x, pe, edge_index, edge_attr, batch):\n",
    "        return self.model(x, pe, edge_index, edge_attr, batch)\n",
    "\n",
    "model = SimpleModel(config_args)\n",
    "state_dict = torch.load(model_path, map_location='cpu')\n",
    "model.load_state_dict(state_dict, strict=False)  # Use strict=False to ignore extra keys\n",
    "model.eval()\n",
    "\n",
    "# You'll also need normalizers - you'll need to load these from your data\n",
    "def denormalize_features(normalized_features, baseMVA=100):\n",
    "    \"\"\"Simple denormalization assuming baseMVA normalization\"\"\"\n",
    "    denormed = normalized_features.copy()\n",
    "    # Power features are normalized by baseMVA\n",
    "    denormed[:, 0:4] *= baseMVA  # PD, QD, PG, QG\n",
    "    return denormed\n",
    "\n",
    "\n",
    "\n",
    "def run_gridfm_inference(pf_data, baseMVA=100):\n",
    "    \"\"\"\n",
    "    Run GridFM inference on a single power flow case.\n",
    "    Reference Code: gridfm-graphkit/gridfm_graphkit/datasets/powergrid_dataset.py (line 140 - 174)\n",
    "    \n",
    "    Args:\n",
    "        pf_data: Dictionary with keys 'bus', 'gen', 'branch', 'Y_bus'\n",
    "        baseMVA: Base MVA for denormalization\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with predicted values\n",
    "    \"\"\"\n",
    "    # Extract node features from bus data\n",
    "    # BUS_COLUMNS: bus, Pd, Qd, Pg, Qg, Vm, Va, PQ, PV, REF, vn_kv, min_vm_pu, max_vm_pu, GS, BS\n",
    "    bus_data = pf_data['bus']\n",
    "    \n",
    "    # Node features: [Pd, Qd, Pg, Qg, Vm, Va, PQ, PV, REF] normalized\n",
    "    node_features = np.column_stack([\n",
    "        bus_data[:, 1] / baseMVA,  # Pd normalized\n",
    "        bus_data[:, 2] / baseMVA,  # Qd normalized\n",
    "        bus_data[:, 3] / baseMVA,  # Pg normalized\n",
    "        bus_data[:, 4] / baseMVA,  # Qg normalized\n",
    "        bus_data[:, 5],  # Vm (already in p.u.)\n",
    "        bus_data[:, 6] / 100,  # Va normalized (assume max 100 degrees)\n",
    "        bus_data[:, 7],  # PQ\n",
    "        bus_data[:, 8],  # PV\n",
    "        bus_data[:, 9],  # REF\n",
    "    ])\n",
    "    \n",
    "\n",
    "    y_bus = pf_data['Y_bus']\n",
    "    edge_index = y_bus[:, 0:2].T.astype(int)  # [2, num_edges]\n",
    "    edge_attr = y_bus[:, 2:4] / baseMVA  # G, B normalized\n",
    "    \n",
    "\n",
    "    edge_index_tensor = torch.tensor(edge_index, dtype=torch.long)\n",
    "    edge_attr_tensor = torch.tensor(edge_attr, dtype=torch.float32)\n",
    "    \n",
    "    # PE\n",
    "    from gridfm_graphkit.datasets.transforms import AddNormalizedRandomWalkPE, AddEdgeWeights\n",
    "    from torch_geometric.data import Data\n",
    "\n",
    "    temp_data = Data(\n",
    "        edge_index=edge_index_tensor,\n",
    "        edge_attr=edge_attr_tensor,\n",
    "        num_nodes=len(bus_data)\n",
    "    )\n",
    "    \n",
    "    pe_pre_transform = AddEdgeWeights() #PE?\n",
    "    temp_data = pe_pre_transform(temp_data) #PE?\n",
    "    pe_transform = AddNormalizedRandomWalkPE(walk_length=20, attr_name=\"pe\") #PE?\n",
    "    temp_data = pe_transform(temp_data) #PE?\n",
    "    pe_tensor = temp_data.pe\n",
    "    \n",
    "\n",
    "    x = torch.tensor(node_features, dtype=torch.float32)\n",
    "    batch = torch.zeros(len(bus_data), dtype=torch.long)\n",
    "    \n",
    "    #inference\n",
    "    with torch.no_grad():\n",
    "        output = model(x, pe_tensor, edge_index_tensor, edge_attr_tensor, batch)\n",
    "        output_np = output.numpy()\n",
    "    \n",
    "    # Denormalize\n",
    "    output_denorm = denormalize_features(output_np, baseMVA)\n",
    "    \n",
    "    return {\n",
    "        'PD_pred': output_denorm[:, 0],\n",
    "        'QD_pred': output_denorm[:, 1],\n",
    "        'PG_pred': output_denorm[:, 2],\n",
    "        'QG_pred': output_denorm[:, 3],\n",
    "        'VM_pred': output_denorm[:, 4],\n",
    "        'VA_pred': output_denorm[:, 5] * 100,  # Convert back to degrees\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4524826",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"net.res_bus\\n\",net.res_bus)\n",
    "print(\"net.res_line\\n\",net.res_line)\n",
    "print(\"net.res_trafo\\n\",net.res_trafo)\n",
    "print(\"net.res_load\\n\",net.res_load)\n",
    "print(\"net.res_gen\\n\",net.res_gen)\n",
    "print(\"net.res_ext_grid\\n\",net.res_ext_grid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gridfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
